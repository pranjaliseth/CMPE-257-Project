# -*- coding: utf-8 -*-
"""random_forest_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YFCAKVe-f9lUwzSTwiw4UpwWI28qiUyY

## Implementing random forest classifier for Online product reviews - rating classification/prediction.
"""

#mounting drive
from google.colab import drive
drive.mount('/content/drive')

#importing the required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.tools as tls
import plotly.offline as py
color = sns.color_palette()
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
import plotly.tools as tls
import warnings
warnings.filterwarnings('ignore')
!pip install scikit-plot
import scikitplot as skplt
import pickle

!pip install stop_words

# NLP modules
import nltk
import re 
import string
from nltk.corpus import stopwords
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
from textblob import TextBlob , Word
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Wordcloud Modules
from wordcloud import WordCloud , STOPWORDS

color = sns.color_palette()
warnings.filterwarnings('ignore')
py.init_notebook_mode(connected=True)
nltk.download("stopwords")
nltk.download("all")

#loading the dataset
reviews_df=pd.read_csv('/content/drive/MyDrive/CMPE_257_Project/amazon_dataset/reviews_data.csv')
reviews_df.head(5)

reviews_df.shape

#Columns/attributes and their datatypes
reviews_df.dtypes

"""### Data cleaning and preprocessing"""

reviews_df.isnull().sum()

#dropping null values from the important columns used for model training

reviews_df = reviews_df.dropna(subset=['reviews.text']) #dropping null reviews text
reviews_df = reviews_df.dropna(subset=['reviews.title']) #dropping null reviews title
reviews_df = reviews_df.dropna(subset=['reviews.rating']) #dropping null ratings

reviews_df.shape

reviews_df.duplicated(subset=['reviews.text', 'reviews.username', 'reviews.rating', 'reviews.date']).sum()

#dropping the duplicated values based on review text, username, rating and date
reviews_df=reviews_df.drop_duplicates(subset=['reviews.text', 'reviews.username', 'reviews.rating', 'reviews.date'])

reviews_df.shape

reviews_df["full_review"] = reviews_df['reviews.title'].astype(str) +" "+ reviews_df["reviews.text"]

# preprocessing the reviews text (converting to lowercase, removing string literals)
reviews_df["full_review"] = (
    reviews_df["full_review"]
    .str.lower()                    
    .str.replace("[^\w\s]", "")
    .str.replace("\d+", "")
    .str.replace("\n", " ")
    .replace("\r", "")
    .str.replace("[^a-zA-Z0-9\s]", "")
)

reviews_df['full_review']

def word_cleaner(data):
    words = [re.sub("[^a-zA-Z]", " ", i) for i in data]
    words = [i.lower() for j in words for i in j.split()] # Split all the sentences into words
    words = [i for i in words if not i in set(stopwords.words("english"))] # Split all the sentences into words
    return words

word_frequency = pd.DataFrame(
    nltk.FreqDist(word_cleaner(reviews_df["full_review"])).most_common(25),
    columns=["Frequent_Words", "Frequency"],
)

#plotting the most frequently used words in the reviews texts.
plt.figure(figsize=(8, 8))
plt.xticks(rotation=90)
plt.title("Most frequently used words in reviews")
sns.barplot(x="Frequent_Words", y="Frequency", data=word_frequency)

# preprocessing reviews text
lemmatizer_output = WordNetLemmatizer()

reviews_df["full_review"] = reviews_df["full_review"].apply(
    lambda x: word_tokenize(x.lower()) # converting the text to lower case
)
reviews_df["full_review"] = reviews_df["full_review"].apply(
    lambda x: [word for word in x if word not in STOPWORDS] #getting rid of stopwords
)
reviews_df["full_review"] = reviews_df["full_review"].apply(
    lambda x: [lemmatizer_output.lemmatize(word) for word in x] #lemmatizes the words in reviews text
)
reviews_df["full_review"] = reviews_df["full_review"].apply(lambda x: " ".join(x))

reviews_df['full_review'].head(15)

#plotting wordcloud
from wordcloud import WordCloud, STOPWORDS

stopwords = set(STOPWORDS)


def show_wordcloud(data, title=None):
    wordcloud = WordCloud(
        background_color="black",
        stopwords=stopwords,
        max_words=250,
        max_font_size=45,
        scale=4,
        random_state=1,
    ).generate(str(data))

    fig = plt.figure(1, figsize=(16, 16))
    plt.axis("off")
    if title:
        fig.suptitle(title, fontsize=21)
        fig.subplots_adjust(top=2.1)

    plt.imshow(wordcloud)
    plt.show()


show_wordcloud(reviews_df["full_review"])

"""### Resampling data"""

!pip install imbalanced-learn

from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.under_sampling import RandomUnderSampler, NearMiss
from collections import Counter

from sklearn.model_selection import cross_val_score
from scipy.sparse import hstack
from sklearn.feature_extraction.text import TfidfVectorizer

whole_text = reviews_df['full_review']
train_text = reviews_df['full_review']
y_ratings = reviews_df['reviews.rating']

#vectorizing the input reviews text
word_vec = TfidfVectorizer(sublinear_tf = True, strip_accents = 'unicode', analyzer = 'word', token_pattern = r'\w{1,}', stop_words = 'english', ngram_range = (1, 1), max_features=10000)
word_vec.fit(whole_text)
train_features = word_vec.transform(train_text)

train_features

#Undersampling using nearmiss

nm = NearMiss()
X_undersample, y_undersample = nm.fit_resample(train_features, y_ratings)

#Oversampling using SMOTE

smote = SMOTE(random_state=42)
X_oversample, y_oversample= smote.fit_resample(train_features, y_ratings)

print('Original dataset shape %s' % Counter(y_ratings))
print('Undersampled dataset shape %s' % Counter(y_undersample))
print('Oversampled dataset shape %s' % Counter(y_oversample))

#creating train and test splits for undersampled, oversampled and original data
from sklearn.model_selection import train_test_split
X_train_us, X_test_us, y_train_us, y_test_us = train_test_split(X_undersample, y_undersample, test_size=0.3, random_state=101)
X_train, X_test, y_train, y_test = train_test_split(train_features, y_ratings, test_size=0.3, random_state=101)
X_train_os, X_test_os, y_train_os, y_test_os = train_test_split(X_oversample, y_oversample, test_size=0.3, random_state=101)

"""### Implementing the random forest classifier and evaluating"""

#training RandomForestClassifier using undersampled data
from sklearn.ensemble import RandomForestClassifier

rfc_us = RandomForestClassifier()
rfc_us.fit(X_train_us,y_train_us)
rfc_pred_us = rfc_us.predict(X_test_us)
print(rfc_pred_us)
rfc_us.score(X_train_us, y_train_us)

#training RandomForestClassifier using original data
rfc = RandomForestClassifier()
rfc.fit(X_train,y_train)
rfc_pred = rfc.predict(X_test)
print(rfc_pred)
rfc.score(X_train, y_train)

#training RandomForestClassifier using oversampled data
rfc_os = RandomForestClassifier()
rfc_os.fit(X_train_os,y_train_os)
rfc_pred_os = rfc_os.predict(X_test_os)
print(rfc_pred_os)
rfc_os.score(X_train_os, y_train_os)

print("Training accuracy score on undersampled data: ", rfc_us.score(X_train_us, y_train_us))
print("Training accuracy score on original data: ", rfc.score(X_train, y_train))
print("Training accuracy score on oversampled data: ", rfc_os.score(X_train_os, y_train_os))

"""To ensure that there is enough  data for building a good model, I applied resampling techniques. 

The training accuracy score for the models is close to a hundred percent. This implies that the error for the models on training is close to zero. This ensures that the model is learned properly. 
"""

# saving the models to use them for any further testing
filename = '/content/drive/MyDrive/CMPE_257_Project/randomforestclassifier_undersampled.sav'
pickle.dump(rfc_us, open(filename, 'wb'))
filename = '/content/drive/MyDrive/CMPE_257_Project/randomforestclassifier.sav'
pickle.dump(rfc, open(filename, 'wb'))
filename = '/content/drive/MyDrive/CMPE_257_Project/randomforestclassifier_oversampled.sav'
pickle.dump(rfc_os, open(filename, 'wb'))

#getting f1 score metrics
from sklearn.metrics import classification_report
print("Classification report for Undersampled data.")
print(classification_report(y_test_us, rfc_pred_us, labels=[1, 2, 3, 4, 5]))
print("\nClassification report for Original (no resampling) data.")
print(classification_report(y_test, rfc_pred, labels=[1, 2, 3, 4, 5]))
print("\nClassification report for Oversampled data.")
print(classification_report(y_test_os, rfc_pred_os, labels=[1, 2, 3, 4, 5]))

"""Though the training accuracy scores were good on undersampled, original and oversampled data. When evaluating on test data, the f1 scores varied a lot.

On oversampled data, the f1 score is significantly better. 

The testing accuracy or f1 score is close to a hundred. This implies that the error for out of training data is close to zero. This ensures that the errors for insample and out of sample data are similar, close to zero and the model is learned properly. 
"""

#plotting confusion matrix
sns.set(rc={'figure.figsize':(10,10)})
sns.set(font_scale=1)
skplt.metrics.plot_confusion_matrix(y_test_us, rfc_pred_us, normalize=True, title = 'Confusion Matrix for RandomForestClassifier (Undersampled)')
plt.show()

#plotting confusion matrix
sns.set(rc={'figure.figsize':(10,10)})
sns.set(font_scale=1)
skplt.metrics.plot_confusion_matrix(y_test, rfc_pred, normalize=True, title = 'Confusion Matrix for RandomForestClassifier (unsampled)')
plt.show()

#plotting confusion matrix
sns.set(rc={'figure.figsize':(10,10)})
sns.set(font_scale=1)
skplt.metrics.plot_confusion_matrix(y_test_os, rfc_pred_os, normalize=True, title = 'Confusion Matrix for RandomForestClassifier (oversampled)')
plt.show()



#plotting the precision recall curve for undersampled data
probas = rfc_us.predict_proba(X_test_us)
sns.set(rc={'figure.figsize':(10,10)})
sns.set(font_scale=1)
skplt.metrics.plot_precision_recall_curve(y_test_us, probas, title = 'Precision-Recall Curve for RandomForestClassifier (undersampled)')
plt.show()

#plotting the precision recall curve for unsampled data
probas2 = rfc.predict_proba(X_test)
sns.set(rc={'figure.figsize':(10,10)})
sns.set(font_scale=1)
skplt.metrics.plot_precision_recall_curve(y_test, probas2, title = 'Precision-Recall Curve for RandomForestClassifier (unsampled)')
plt.show()

#plotting the precision recall curve for oversampled data
probas3 = rfc_os.predict_proba(X_test_os)
sns.set(rc={'figure.figsize':(10,10)})
sns.set(font_scale=1)
skplt.metrics.plot_precision_recall_curve(y_test_os, probas3, title = 'Precision-Recall Curve for RandomForestClassifier (oversampled)')
plt.show()

"""The precision-recall on each class label for randomforest classifier on oversampled data is much more consistent and higher than the other versions of the classifier."""

# calculating the log loss (since we have dealt with imbalanced multiclass data, log loss is a good choice)
from sklearn.metrics import log_loss
probas_us = rfc_us.predict_proba(X_test_us)
probas_ = rfc.predict_proba(X_test)
print("Log loss for undersampled data on RandomForestClassifier")
print(log_loss(y_test_us, probas_us))
print("\nLog loss for original (no resampling) data on RandomForestClassifier")
print(log_loss(y_test, probas_))
print("\nLog loss for oversampled data on RandomForestClassifier")
print(log_loss(y_test_os, probas3))

"""The loss for randomforestclassifier on oversampled data is a lot less comparitively, implying that the model on the oversampled data is trained well.

### Custom test cases
"""

# giving some custom test inputs to evaluate the model
custom_test_inputs = ["so satisfied with the purchase good product works well", "this device feels ok it works fine", "really disappointed with the purchase defective product not working", "used to be good but since the change the worst product ever", "used to be bad but from when it was updated it is the best product ever"]
inputs_vec = word_vec.transform(custom_test_inputs)

# testing on rfc_os model (randomforestclassifier model that was trained on the oversampled data.)
custom_preds = rfc_os.predict(inputs_vec)
for index in range(len(custom_test_inputs)):
  print("The rating predicted for the review - \"", custom_test_inputs[index], "\" is : ", custom_preds[index])

"""The rating predictions on this version of the model (on oversampled data) are pretty good/accurate. """

# testing on rfc model (randomforestclassifier model that was trained on the original data.)
custom_preds = rfc.predict(inputs_vec)
for index in range(len(custom_test_inputs)):
  print("The rating predicted for the review - \"", custom_test_inputs[index], "\" is : ", custom_preds[index])

"""The rating predictions for this version of the model (on unsampled data) are not up to the mark."""

