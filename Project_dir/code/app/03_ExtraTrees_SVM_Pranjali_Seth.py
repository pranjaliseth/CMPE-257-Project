# -*- coding: utf-8 -*-
"""Final_Project_ExtraTrees_SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cJi-xXUNWPXozLntBQYPddZCI8SunGNN
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.tools as tls
import plotly.offline as py
color = sns.color_palette()
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
import plotly.tools as tls
import warnings
warnings.filterwarnings('ignore')
!pip install scikit-plot
import scikitplot as skplt
import pickle

!pip install stop_words

# NLP modules
import nltk
import re 
import string
from nltk.corpus import stopwords
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
from textblob import TextBlob , Word
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Wordcloud Modules
from wordcloud import WordCloud , STOPWORDS

color = sns.color_palette()
warnings.filterwarnings('ignore')
py.init_notebook_mode(connected=True)
nltk.download("stopwords")
nltk.download("all")

reviews_df=pd.read_csv('/content/drive/MyDrive/CMPE_257_Project/amazon_dataset/reviews_data.csv')
reviews_df.head(5)

reviews_df.shape

#Columns/attributes and their datatypes
reviews_df.dtypes

reviews_df.isnull().sum()

reviews_df = reviews_df.dropna(subset=['reviews.text']) #dropping null reviews text
reviews_df = reviews_df.dropna(subset=['reviews.title']) #dropping null reviews title
reviews_df = reviews_df.dropna(subset=['reviews.rating']) #dropping null ratings

reviews_df.shape

reviews_df.duplicated(subset=['reviews.text', 'reviews.username', 'reviews.rating', 'reviews.date']).sum()

reviews_df=reviews_df.drop_duplicates(subset=['reviews.text', 'reviews.username', 'reviews.rating', 'reviews.date'])

reviews_df.shape

reviews_df["full_review"] = reviews_df['reviews.title'].astype(str) +" "+ reviews_df["reviews.text"]

reviews_df["full_review"] = (
    reviews_df["full_review"]
    .str.lower()
    .str.replace("[^\w\s]", "")
    .str.replace("\d+", "")
    .str.replace("\n", " ")
    .replace("\r", "")
    .str.replace("[^a-zA-Z0-9\s]", "")
)

reviews_df['full_review']

def word_cleaner(data):
    words = [re.sub("[^a-zA-Z]", " ", i) for i in data]
    words = [i.lower() for j in words for i in j.split()] # Split all the sentences into words
    words = [i for i in words if not i in set(stopwords.words("english"))] # Split all the sentences into words
    return words

word_frequency = pd.DataFrame(
    nltk.FreqDist(word_cleaner(reviews_df["full_review"])).most_common(25),
    columns=["Frequent_Words", "Frequency"],
)

plt.figure(figsize=(8, 8))
plt.xticks(rotation=90)
plt.title("Most frequently used words in reviews")
sns.barplot(x="Frequent_Words", y="Frequency", data=word_frequency)

lemmatizer_output = WordNetLemmatizer()

reviews_df["full_review"] = reviews_df["full_review"].apply(
    lambda x: word_tokenize(x.lower())
)
reviews_df["full_review"] = reviews_df["full_review"].apply(
    lambda x: [word for word in x if word not in STOPWORDS]
)
reviews_df["full_review"] = reviews_df["full_review"].apply(
    lambda x: [lemmatizer_output.lemmatize(word) for word in x]
)
reviews_df["full_review"] = reviews_df["full_review"].apply(lambda x: " ".join(x))

reviews_df['full_review'].head(15)

from wordcloud import WordCloud, STOPWORDS

stopwords = set(STOPWORDS)


def show_wordcloud(data, title=None):
    wordcloud = WordCloud(
        background_color="black",
        stopwords=stopwords,
        max_words=250,
        max_font_size=45,
        scale=4,
        random_state=1,
    ).generate(str(data))

    fig = plt.figure(1, figsize=(16, 16))
    plt.axis("off")
    if title:
        fig.suptitle(title, fontsize=21)
        fig.subplots_adjust(top=2.1)

    plt.imshow(wordcloud)
    plt.show()


show_wordcloud(reviews_df["full_review"])

plt.figure(figsize=(8,8))
sns.histplot(data=reviews_df, x=reviews_df['reviews.rating'], discrete="True").set(title = "Frequency of each rating")

#review by brand
reviews_df.groupby(reviews_df['brand']).mean()['reviews.rating']

reviews_df["reviews_length"] = reviews_df["reviews.text"].apply(len)
sns.set(font_scale=2.0)

graph = sns.FacetGrid(reviews_df,col='reviews.rating',size=5)
graph.map(plt.hist,'reviews_length', range=[0, 500])

reviews_df['reviews.doRecommend'].fillna("N/A",inplace=True)

plt.figure(figsize = (8,8))
plt.title("Product recommendation from reviews")
reviews_df["reviews.doRecommend"].value_counts().plot.pie(autopct="%1.1f%%",textprops={'fontsize': 18})

plt.figure(figsize=(12,8))
plt.hist(reviews_df['reviews.numHelpful'],range=[1, 25], orientation='horizontal')
plt.title("Helpfulness of the reviews")
plt.xlabel("Count", fontsize=12)
plt.ylabel("No. of people that found the review helpful", fontsize=12)

sns.set(font_scale=1.4)
plt.figure(figsize = (10,5))
plt.title("Heat map - Correlation")
sns.heatmap(reviews_df.corr(),cmap='coolwarm',annot=True,linewidths=.5)





!pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import NearMiss
from collections import Counter

from sklearn.feature_extraction.text import TfidfVectorizer

whole_text = reviews_df['full_review']
train_text = reviews_df['full_review']
y_ratings = reviews_df['reviews.rating']

word_vec = TfidfVectorizer(sublinear_tf = True, strip_accents = 'unicode', analyzer = 'word', token_pattern = r'\w{1,}', stop_words = 'english', ngram_range = (1, 1), max_features=10000)
word_vec.fit(whole_text)
train_features = word_vec.transform(train_text)

train_features

#Undersampling

nm = NearMiss()
X_undersample, y_undersample = nm.fit_resample(train_features, y_ratings)

#Oversampling

smote = SMOTE(random_state=42)
X_oversample, y_oversample= smote.fit_resample(train_features, y_ratings)

print('Original dataset shape %s' % Counter(y_ratings))
print('Undersampled dataset shape %s' % Counter(y_undersample))
print('Oversampled dataset shape %s' % Counter(y_oversample))

from sklearn.model_selection import train_test_split
X_train_us, X_test_us, y_train_us, y_test_us = train_test_split(X_undersample, y_undersample, test_size=0.3, random_state=101)
X_train, X_test, y_train, y_test = train_test_split(train_features, y_ratings, test_size=0.3, random_state=101)
X_train_os, X_test_os, y_train_os, y_test_os = train_test_split(X_oversample, y_oversample, test_size=0.3, random_state=101)

"""ExtraTreesClassifier"""

from sklearn.ensemble import ExtraTreesClassifier

etc_us = ExtraTreesClassifier(n_estimators=100, random_state=0)
etc_us.fit(X_train_us,y_train_us)
etc_pred_us = etc_us.predict(X_test_us)
print(etc_pred_us)
etc_us.score(X_train_us, y_train_us)

etc = ExtraTreesClassifier(n_estimators=100, random_state=0)
etc.fit(X_train,y_train)
etc_pred = etc.predict(X_test)
print(etc_pred)
etc.score(X_train, y_train)

etc_os = ExtraTreesClassifier(n_estimators=100, random_state=0)
etc_os.fit(X_train_os,y_train_os)
etc_pred_os = etc_os.predict(X_test_os)
print(etc_pred_os)
etc_os.score(X_train_os, y_train_os)



from sklearn.metrics import classification_report
print("Classification report for Undersampled data using ExtraTreeClassifier.")
print(classification_report(y_test_us, etc_pred_us, labels=[1, 2, 3, 4, 5]))
print("\nClassification report for Original (no resampling) data using ExtraTreeClassifier.")
print(classification_report(y_test, etc_pred, labels=[1, 2, 3, 4, 5]))
print("\nClassification report for Oversampled data using ExtraTreeClassifier.")
print(classification_report(y_test_os, etc_pred_os, labels=[1, 2, 3, 4, 5]))

sns.set(rc={'figure.figsize':(10,10)})
sns.set(font_scale=1)
skplt.metrics.plot_confusion_matrix(y_test_os, etc_pred_os, normalize=True, title = 'Confusion Matrix for Extratreeclassifier (oversampled)')
plt.show()

probas3 = etc_os.predict_proba(X_test_os)
sns.set(rc={'figure.figsize':(10,10)})
sns.set(font_scale=1)
skplt.metrics.plot_precision_recall_curve(y_test_os, probas3)
plt.show()

from sklearn.metrics import log_loss
probas3_us = etc_us.predict_proba(X_test_us)
probas3_ = etc.predict_proba(X_test)
print("Log loss for undersampled data on ExtraTreeClassifier")
print(log_loss(y_test_us, probas3_us))
print("\nLog loss for original (no resampling) data on ExtraTreeClassifier")
print(log_loss(y_test, probas3_))
print("\nLog loss for oversampled data on ExtraTreeClassifier")
print(log_loss(y_test_os, probas3))



"""SVM - SVClassifier"""

from sklearn.svm import SVC
svc_us = SVC(kernel='linear', random_state=32)
svc_us.fit(X_train_us,y_train_us)
svc_pred_us = svc_us.predict(X_test_us)
print(svc_pred_us)
svc_us.score(X_train_us, y_train_us)

svc = SVC(kernel='linear', random_state=32)
svc.fit(X_train,y_train)
svc_pred = svc.predict(X_test)
print(svc_pred)
svc.score(X_train, y_train)

from sklearn.metrics import classification_report
print("Classification report for Undersampled data using SVC.")
print(classification_report(y_test_us, svc_pred_us, labels=[1, 2, 3, 4, 5]))
print("\nClassification report for Original (no resampling) data using SVC.")
print(classification_report(y_test, svc_pred, labels=[1, 2, 3, 4, 5]))



"""Custom test cases"""

custom_test = word_vec.transform(["so satisfied with the purchase good product works well", "this device feels ok it works fine", "really disappointed with the purchase defective product not working", "used to be good but since the change the worst product ever", "used to be bad but from when it was updated it is the best product ever"])

etc_us.predict(custom_test)

etc.predict(custom_test)

etc_os.predict(custom_test)

svc_us.predict(custom_test)

svc.predict(custom_test)

